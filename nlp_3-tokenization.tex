\documentclass[style=upen, size=14pt]{powerdot}
\definecolor{arany}{RGB}{255,242,0}
\hypersetup{backref=page}
\hypersetup{
    colorlinks=true,
    linkcolor=cyan,
    filecolor=magenta,      
    urlcolor=cyan}
\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage[latin2]{inputenc}
%\usepackage[magyar]{babel}
%\usepackage{euler}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{tikz-dependency}
\usepackage{linguex}
\usepackage{amsthm}

\tikzset{every tree node/.style={align=center,anchor=north}}
%\usepackage{tabularx}
%\usepackage{threeparttable}
%\usepackage{color}
%\selectlanguage{english}
%\frenchspacing
\newcommand{\nd}{\noindent}
\newcommand{\Val}{\mathop{\mathit{Val}}}
\newcommand{\gold}{\color{arany}}
%\usepackage{tikz}
%\usepackage{tikz-qtree}
%\newcommand{\qed}{\hfill\mbox{\raggedright \rule{.1in}{.1in}}}
\def\es{\mathbin\land}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{axioma}{Axiom}
\newtheorem{tetel}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\begin{document}

\title{Natural Language Processing\\~~\\Lecture 3\\Tokenization}
% \author{}

\date{2021}
\maketitle

\begin{slide}[toc=Whitespace splitting]{Baseline: splitting on whitespace}
  For many writing systems, splitting the text at \emph{white spaces} is a
  useful baseline:\bigskip

  'This isn't an easy sentence to tokenize!' $\Rightarrow$ \\
  
  ['This', "isn't", 'an', 'easy', 'sentence', 'to', 'tokenize!']\bigskip

  Problems:
  \begin{itemize}
  \item we typically want to treat punctuation marks as separate tokens (but
    only if they are really punctuation, think of 'U.K.' or '10,000.00\$'); 
  \item this solution cannot separate token pairs without white space between
    them, e.g.,  expressions with clitics like "isn't".
  \end{itemize}
\end{slide}

\begin{slide}[toc=Regular expressions]{Regular expressions}
  We need to introduce more sophisticated patterns to describe token boundaries
  in context-dependent way. A popular solution is using \emph{\gold regular
    expressions} (regexes for short).\bigskip

  Given a finite $\Sigma$ alphabet of symbols, regexes over $\Sigma$ and their
  matches in $\Sigma^*$ are defined by simultaneous recursion as follows:
  \begin{enumerate} 
  \item The empty string and any single symbol in $\Sigma$ is a regex
    over $\Sigma$ and matches itself.
  \end{enumerate}
\end{slide}

\begin{slide}[toc=]{Regular expressions cont.}
  \begin{enumerate}
     \setcounter{enumi}{1}
  \item If $r_1$ and $r_2$ are regexes over $\Sigma$ then
    \begin{enumerate}
    \item their \emph{\gold concatenation}, $r_1 r_2$ is also a regex over $\Sigma$ and
      matches exactly those strings that are the concatenation of a string
      matching $r_1$ and a string matching $r_2$, and
    \item their \emph{\gold alternation}, $r_1 \vert r_2$ is also a regex over
      $\Sigma$ and matches exactly those strings that match either $r_1$ or
      $r_2$.
    \end{enumerate}
  \item If $r$ is a regex over $\Sigma$ then applying the \emph{\gold Kleene
      star} operator to $r$ we can form a new regex $r^*$ which matches exactly
    those strings that are the concatenation of 0 or more strings each
    matching $r$.
  \end{enumerate}
\end{slide}

\begin{slide}[toc=]{Regular expressions cont.}
  A formal language $\mathcal L$ (defined simply as an arbitrary set of strings
  in a finite alphabet) is a \emph{\gold regular language} iff there exists a
  regular expression that matches exactly $\mathcal L$'s elements.\bigskip

  There are simple formal languages that are not regular, e.g., the ``twin
  language'' $\{ww ~\vert~ w \in \{a, b\}^* \}$.\bigskip

  Nonetheless, regular expressions are flexible enough for a lot of practical
  tasks, and there are higly efficient algorithms for deciding whether an $s$
  string matches a regex (with $\mathcal O(\mathrm{length}(s))$ time
  complexity).
\end{slide}


\begin{slide}[toc=]{Regular expressions: extensions}
  \emph{\gold Convenience extensions} not increasing expressive power just adding useful
  shortcuts, e.g.;
  \begin{itemize}
  \item character classes matching any single symbol in a set, e.g. [0-9];
  \item complemented character classes matching any character \emph{not} in the
    complemented set, e.g. [\^{}ab];
  \item operator for optional  matching: $r?$ matches the empty string or a match of $r$;
  \item operators for specifying the required number of pattern repetitions,
    e.g., $r\{m,n\}$ matches $s$ if $s$ repeats the $r$ pattern $k$ times, with
    $m\leq k \leq n$.
  \end{itemize}
\end{slide}

\begin{slide}[toc=]{Regular expressions: back-references}
  So-called \emph{\gold back-reference} constructions, in contrast, that allow
  naming and referencing match(es) corresponding to earlier parts of the regex
  \emph{increase} the expressive power.\bigskip

  For example, most current regex libraries allow a regex similar to\bigskip

  \begin{center}
    (?P$<$a$>$[ab]*)(?P=a)\bigskip
  \end{center}

  which uses back-reference to define exactly the aforementioned non-regular
  ``twin language''.
\end{slide}

\begin{slide}[toc=]{Regex-based find and replace}
  In addition to matching whole strings against a regex, two regex-based tasks are
  very common:
  \begin{itemize}
  \item finding substrings of a string that match a regex,
  \item regex-based find-and-replace: in its simplest form this is replacing
    matching substrings with a given string, but two notable
    extras are provided by modern regex libraries:
    \begin{itemize}
    \item the regex can have look-ahead and look-back parts, that are used when
      finding a match but do not count in the part which is replaced;
    \item replacements do not have to be fix -- they can contain back-references
      to parts of the match.
    \end{itemize}
  \end{itemize}
\end{slide}

\begin{slide}[toc=Regex cascade]{Regex cascade-based tokenization}
  Core idea: perform regex-based substitutions on the input so that in the end
  it's enough to split on white spaces.\bigskip

  The
  \href{ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenizer.sed}{tokenizer
    sed script} accompanying the Penn Tree Bank is a good example. A few
  representative rules (\textbackslash \& refers back to the full match,
  \textbackslash $n$ to the $n$-th group):\bigskip

  '...' $\Rightarrow$ ' ... ' (separate ellipsis)
  
  '[,;:\#\$\%\&]' $\Rightarrow$ ' \textbackslash \& ' (separate various signs)

%   # Assume sentence tokenization has been done first, so split FINAL periods
%   # only.
  
  \textbackslash([\^{}.]\textbackslash)\textbackslash([.]\textbackslash)\textbackslash([])\}"']*\textbackslash)[   ]*\$ $\Rightarrow$ '\textbackslash 1 \textbackslash 2\textbackslash 3'

  (assume sentence input and split FINAL periods only)

  "'ll" $\Rightarrow$ " 'll" (separate clitic 'll)
\end{slide}

\begin{slide}[toc=]{Regex cascade-based tokenization cont.}
  The main problem for the approach is the proper handling of exceptions: e.g.,
  word ending periods should be split, \emph{except for abbreviations}.\bigskip

  The standard solution is to replace the problematic expressions with
  unproblematic placeholders before executing the substitutions in question, e.g.\smallskip

  \begin{center}
    (etc\textbackslash.$\vert$i\textbackslash.e\textbackslash.$\vert$e\textbackslash.g\textbackslash.)
    $\Rightarrow$ $<$abbrev$>$
  \end{center}\smallskip
  
  This solution requires keeping track of the placeholder substitutions and
  restoring the originals after executing the problematic rules.
\end{slide}

\begin{slide}[toc=Lexers]{Lexer-based solutions}
  They use off-the shelf ``lexers'' (lexical analyzers), originally developed
  for the tokenization/lexical analysis of computer programs.

  A typical lexer takes a character stream as input and produces a stream of
  classified tokens from it:

  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/lexer.eps}
  \end{center}
\end{slide}

\begin{slide}[toc=]{Lexer-based solutions cont.}
  Most lexers are actually lexical analyser generators. Their input is a list of
  token classes (types), regular expression patterns and\smallskip

  \begin{center}
    [\textsc{RegexPattern}] $\Rightarrow$ [\textsc{Action}]\smallskip
  \end{center}

rules (where the most important action is classifying the actual match as a
token of a given type), and they generate a concrete, optimized lexical analyzer
implementing the given rules, e.g., by generating the C source code of the
analyzer.
\end{slide}

\begin{slide}[toc=spaCy]{Case study: spaCy's rule-based tokenizer}
  % To appreciate how a purpose-built, rule-based NLP tokenizer might look like,
  % let's look briefly at spaCy's built in tokenizer. The following explanation is
  % from the spaCy website's tokenization section:

  \begin{enumerate}
  \item The input text is split on white space.\footnote{The algorithm
    description is from the
    \href{https://spacy.io/usage/linguistic-features\#tokenization}{spaCy
      documentation.}}
  \item Then, the tokenizer processes the text from left to right. On each
    substring, it performs two checks:
    \begin{enumerate}
    \item Does the substring match a tokenizer exception rule? For example,
      "don't" does not contain whitespace, but should be split.
    \item Can a prefix, suffix or infix be split off? For example punctuation
      like commas, periods.
    \end{enumerate}
    If there's a match, the rule is applied and the tokenizer continues
    its loop, starting with the newly split substrings.
  \end{enumerate}

\end{slide}

\begin{slide}[toc=]{SpaCy's rule-based tokenizer cont.}
  A simple example: tokenizing \emph{``Let's go to N.Y.!''}
    \begin{center}
      \includegraphics[width=0.9\textwidth]{figures/spacy_tokenizer.eps}\
      
      \footnotesize{(Figure from the \href{https://spacy.io/usage/linguistic-features\#tokenization}{spaCy documentation})}
    \end{center}
\end{slide}

\begin{slide}[toc=Edit distance]{Edit distance}
  In addition to segmenting the input into units, tokenization also involves
  classifying tokens into types, deciding e.g., which type(s)\smallskip
  \begin{center}
    'Apple', 'apple', 'appple'\smallskip
  \end{center}
  belong to. In many cases, these decisions require a \emph{similarity metric}
  between strings.

  One of the most important metric families in this domain is the so-called
  \emph{\gold edit distance} family, which measure the distance between two
  strings by the number edit operations required to transform them into each
  other.
\end{slide}

\begin{slide}[toc=]{Edit distance cont.}
  Given
  \begin{itemize}
  \item a set of \emph{\gold editing operations} (e.g., removing or inserting a character
    from/into the string) and
  \item a \emph{\gold weight function} that assigns a weight to each operation,
  \end{itemize}
  the \emph{\gold edit distance} between two strings, a source and a target, is
  the minimum total weight that is needed for transforming the source into the
  target.
\end{slide}
    
\begin{slide}[toc=]{Levenshtein distance}
  One of the most important variants is the so-called Levenshtein distance,
  where the operations are the
   \begin{itemize}
  \item deletion,
  \item insertion, and
  \item substitution of a character,
  \end{itemize}
  and the weight of all operations is 1.0.
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/levenshtein.eps}\
    
    \footnotesize{(Figure from \href{https://devopedia.org/levenshtein-distance}{Devopedia})}
  \end{center}
\end{slide}


\begin{slide}[toc=]{Regular expressions: back-references}
  
\end{slide}


\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

% LocalWords:  Tokenization
